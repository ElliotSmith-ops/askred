import type { NextApiRequest, NextApiResponse } from "next";
import OpenAI from "openai";
import axios from "axios";
import { createClient } from "@supabase/supabase-js";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);
const SERPAPI_KEY = process.env.SERPAPI_KEY!;

type SerpResult = {
  title: string;
  link: string;
};

type RedditComment = {
  kind: string;
  data: {
    body: string;
  };
};

type GPTProduct = {
  product: string;
  reason: string;
  endorsement_score?: number;
  redditUrl: string;
  amazonUrl: string;
};

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const rawQuery = req.body.query;
  const query = rawQuery?.trim().toLowerCase();
  if (!query) return res.status(400).json({ error: "Missing query" });

  try {
    console.log("ğŸ” Checking Supabase for query:", query);
    const { data: cachedList, error: cacheError } = await supabase
      .from("search_queries")
      .select("gpt_result, reddit_urls")
      .eq("query", query);

    if (cacheError) console.error("âŒ Supabase cache check error:", cacheError);
    const cached = cachedList?.[0];

    if (cached && cached.gpt_result) {
      console.log("âœ… Cache hit. Returning cached results.");
      return res.status(200).json({ results: cached.gpt_result, posts: cached.reddit_urls || [] });
    }

    console.log("âš¡ Searching via SerpAPI (google_light) for:", query);

    const serpUrl = `https://serpapi.com/search.json?q=${encodeURIComponent(query + " product recommendations site:reddit.com")}&api_key=${SERPAPI_KEY}&num=10`;
    const serpResponse = await axios.get(serpUrl);
    const serpResults: SerpResult[] = serpResponse.data?.organic_results || [];

    console.log("ğŸ” google_light results found:", serpResults.length);
    serpResults.forEach((r: SerpResult, i: number) => {
      console.log(`  ${i + 1}. ${r.title} â€” ${r.link}`);
    });

    const threads = serpResults
      .filter((r: SerpResult) => r.link?.includes("reddit.com/r/"))
      .map((r: SerpResult) => ({
        title: r.title,
        url: r.link,
        subreddit: r.link.split("/r/")[1]?.split("/")[0] || "reddit",
        score: 0,
        num_comments: 0,
      }));

    async function processThread(thread: { title: string; url: string; subreddit: string; score: number; num_comments: number }): Promise<GPTProduct[]> {
      console.log("\n==============================");
      console.log("ğŸ“„ Processing thread:", thread.url);

      try {
        const postIdMatch = thread.url.match(/comments\/(\w+)/);
        const postId = postIdMatch?.[1];
        console.log("ğŸ” Extracted postId:", postId);

        if (!postId) {
          console.warn("âš ï¸ Skipping â€” no postId found.");
          return [];
        }

        console.log("ğŸŒ Fetching Reddit thread JSON...");
        const redditResponse = await axios.get(`https://www.reddit.com/comments/${postId}.json`, {
          headers: { "User-Agent": "AskRedApp/1.0" },
        });

        const dataLayer = redditResponse.data?.[1]?.data;
        const commentsRaw: RedditComment[] = dataLayer?.children;
        console.log("ğŸ’¬ Raw comment count:", commentsRaw?.length);

        const topComments = (commentsRaw || [])
          .filter((c: RedditComment) => c.kind === "t1" && c.data?.body)
          .map((c: RedditComment) => c.data.body)
          .slice(0, 15);

        console.log("ğŸ’¬ Filtered top comment count:", topComments.length);

        if (topComments.length === 0) {
          console.warn("âš ï¸ Skipping â€” no usable top comments.");
          return [];
        }

        const commentBlock = topComments.map((c: string, i: number) => `${i + 1}. ${c}`).join("\n\n");
        console.log("ğŸ““ Comment block preview (first 300 chars):", commentBlock.slice(0, 300));

        const prompt = `
You are an assistant extracting only **clearly endorsed product recommendations** from Reddit comments about "${query}".

Only include products that are explicitly recommended or praised as something the commenter has **personally used** or strongly supports.

Skip vague mentions, jokes, comparisons, speculation, or off-topic products. Itâ€™s perfectly acceptable to return an empty list if no clear recommendations are found.

For each recommendation, return:
- "product": The name of the product being recommended.
- "reason": A brief explanation of why users recommended it.
- "endorsement_score": A number from 0 to 1 representing the strength of the endorsement:
  - 0.9â€“1.0 = Strong, repeated, enthusiastic endorsements by multiple users
  - 0.6â€“0.8 = Recommended clearly by at least one user
  - 0.3â€“0.5 = Mentioned with some endorsement but less certainty or consensus
  - 0.0â€“0.2 = Do not include these

Output must be valid JSON â€” no markdown, no intro, no trailing comments. Return only the array.

Example:
[
  {
    "product": "Firestone Pond Liner",
    "reason": "Multiple users said it's durable, UV-resistant, and fish-safe.",
    "endorsement_score": 0.94
  }
]

Comments:
${commentBlock}`.trim();

        console.log("ğŸ¤– Calling GPT...");
        const completion = await openai.chat.completions.create({
          model: "gpt-3.5-turbo",
          temperature: 0.7,
          messages: [
            {
              role: "system",
              content: "You extract product recommendations from Reddit comments. Return ONLY valid JSON. No markdown, no explanation, no text before or after the array.",
            },
            {
              role: "user",
              content: prompt,
            },
          ],
        });

        const raw = completion.choices?.[0]?.message?.content?.trim() || "[]";
        console.log("ğŸ¨  GPT Raw Output (first 300 chars):", raw.slice(0, 300));

        let parsed: GPTProduct[] = [];
        try {
          const jsonStart = raw.indexOf("[");
          const jsonEnd = raw.lastIndexOf("]");
          const cleanJson = raw.slice(jsonStart, jsonEnd + 1);

          parsed = JSON.parse(cleanJson).map((item: GPTProduct) => ({
            product: item.product,
            reason: item.reason,
            endorsement_score: item.endorsement_score || null,
            redditUrl: thread.url,
            amazonUrl: `https://www.amazon.com/s?k=${encodeURIComponent(item.product)}&tag=askred-20`,
          }));

          console.log("âœ… Parsed product count:", parsed.length);
        } catch (jsonErr) {
          console.error("ğŸ” JSON parse error:", jsonErr);
          console.error("ğŸ“ GPT raw output that failed:", raw);
          return [];
        }

        return parsed;
      } catch (threadErr) {
        console.error("âŒ Error processing thread:", thread.url, "\n", threadErr);
        return [];
      }
    }

    const threadPromises = threads.slice(0, 5).map(processThread);
    const parsedArrays = await Promise.all(threadPromises);
    const allParsedResults = parsedArrays.flat().filter(Boolean);

    console.log("ğŸ“¦ Total parsed product results:", allParsedResults.length);

    const { error: insertError } = await supabase.from("search_queries").insert({
      query,
      reddit_urls: threads,
      gpt_result: allParsedResults,
      last_updated: new Date().toISOString(),
    });

    if (insertError) {
      console.error("âŒ Supabase insert error:", insertError);
    } else {
      console.log("ğŸ“… Supabase insert successful.");
    }

    res.status(200).json({ results: allParsedResults, posts: threads });
  } catch (err) {
    console.error("ğŸ”¥ Fatal error in /api/search:", err);
    res.status(500).json({ error: "Search failed" });
  }
}
